\newpage

\section[Day 3: Extrema]{ Extrema }

\subsection{ Taylor's Theorem }

    \begin{wtheorem}{Taylor's Theorem for one variable}{14cm}
        Let f: X $\subset$ $\mathbb{R}$ $\rightarrow$ $\mathbb{R}$
        be $C^{k+1}$.
        
        Then for a $\in$ X, the k-th order Taylor polynomial of f centered at a:

        \hspace{0.5cm}
        $p_k(x)$
        = $f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2
            + ... + \frac{f^{(k)}(a)}{k!}(x-a)^k$

        Then for h $>$ 0, there is a x $\in$ (a,a+h) such that:

        \hspace{0.5cm}
        f(a+h) = $p_k(a+h)$ + $\frac{f^{(k+1)}(x)}{(k+1)!}h^{k+1}$
    \end{wtheorem}

    \begin{proof}
        Let M be defined such that f(a+h) = $p_k(a+h)$ + $M(a+h-a)^{k+1}$.

        Let g(t) = f(t) - $p_k(t)$ - $M(t-a)^{k+1}$ for t $\in$ [a,a+h]. Then:

        \hspace{0.5cm}
        $g^{(k+1)}(t)$ = $f^{(k+1)}(t) - (k+1)!M$

        Since $p_k^{(i)}(a)$ = $f^{(i)}(a)$ for i = \{0,...,k\}, then:
        
        \hspace{0.5cm}
        $g^{(i)}(a)$ = $f^{(i)}(a)$ - $p_k^{(i)}(a)$
                        - $\frac{(k+1)!}{(k+1-i)!}M(a-a)^{k+1-i}$ = 0

        Since g(a+h) = 0 where f is differentiable so g is differentiable,
        then by the Mean Value Theorem, there is a $t_1$ $\in$ (0,h):

        \hspace{0.5cm}
        0 - 0
        = g(a+h) - g(a)
        = h*g'($a+t_1$)
        \hspace{0.5cm}
        $\Rightarrow$
        \hspace{0.5cm}
        g'($a+t_1$) = 0

        Since g' is differentiable, by the Mean Value Theorem,
        there is a $t_2$ $\in$ $(0,t_1)$:

        \hspace{0.5cm}
        0 - 0
        = g'($a+t_1$) - g'(a)
        = $t_1$*g''($a+t_2$)
        \hspace{0.5cm}
        $\Rightarrow$
        \hspace{0.5cm}
        g'($a+t_2$) = 0

        Repeating the process k+1 times, there is a $t_{k+1}$ $\in$ $(0,t_k)$:

        \hspace{0.5cm}
        0 - 0
        = $g^{(k)}(a+t_k)$ - $g^{(k)}(a)$
        = $t_k$*$g^{(k+1)}(a+t_{k+1})$
        \hspace{0.5cm}
        $\Rightarrow$
        \hspace{0.5cm}
        $g^{(k+1)}(a+t_{k+1})$ = 0

        Since $t_{k+1}$ $<$ $t_k$ $<$ ... $<$ $t_1$ $<$ h, then:

        \hspace{0.5cm}
        0 = $g^{(k+1)}(a+t_{k+1})$ = $f^{(k+1)}(a+t_{k+1}) - (k+1)!M$

        Thus, M = $\frac{f^{(k+1)}(a+t_{k+1})}{(k+1)!}$
        where $a+t_{k+1}$ $\in$ [a,a+h].
    \end{proof}

    \vspace{0.5cm}



    \begin{wtheorem}{Taylor's Theorem for multiple variables}{14cm}
        Let f: X $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}$
        be $C^{k+1}$. Then for a $\in$ X:

        \hspace{0.5cm}
        $p_k(x)$
        = $f(a)$
            + $\sum_{i=1}^n f_{x_i}(a)(x_i - a_i)$
            + $\frac{1}{2} \sum_{i,j=1}^n f_{x_ix_j}(a)(x_i - a_i)(x_j - a_j)$

            \hspace{2cm}
            + ...
            + $\frac{1}{k!} \sum_{i_1,...,i_k=1}^n
                f_{x_{i_1}...x_{i_k}}(a)(x_{i_1} - a_{i_1})...(x_{i_k} - a_{i_k})$

        Then for $h > 0$:

        \hspace{0.5cm}
        f(a+h) = $p_k(a+h)$ + $R(a)$
        \hspace{1cm}
        where $\lim_{x \rightarrow a}$ $\frac{R(a)}{||x-a||}$ = 0
    \end{wtheorem}

    \begin{proof}
        Let g(t) = f(a+t(x-a)).
        Since f is $C^{k+1}$, then each $D^if$ for i = \{1,...,k+1\} exist
        and by {\color{red} theorem 2.3.10}:

        \hspace{0.5cm}
        $g'(t)$
        = Df(a+t(x-a)) (x-a)
        = $\sum_{i=1}^n$ $f_{x_i}(a+t(x-a))(x_i-a_i)$

        \hspace{0.5cm}
        $g''(t)$
        = $\sum_{i=1}^n \sum_{j=1}^n$
            $f_{x_ix_j}(a+t(x-a))(x_j-a_j)(x_i-a_i)$

        \hspace{0.5cm}
        $g'''(t)$
        = $\sum_{i=1}^n \sum_{j=1}^n \sum_{k=1}^n$
            $f_{x_ix_jx_k}(a+t(x-a))(x_k-a_k)(x_j-a_j)(x_i-a_i)$

        \hspace{0.5cm}
        etc

        Since f is $C^{k+1}$, then g is $C^{k+1}$ and by {\color{red} theorem 3.1.1},
        there is a t $\in$ (0,1) such that:

        \hspace{0.5cm}
        $g(1)$ = $g(0) + g'(0)1 + \frac{g''(0)}{2!}1^2
                    + ... + \frac{g^{(k)}(0)}{k!}1^k
                    + \frac{g^{(k+1)}(t)}{(k+1)!}1^{k+1}$

        \hspace{0.5cm}
        f(x) = f(a) + $\sum_{i=1}^n$ $f_{x_i}(a)(x_i-a_i)$
                + $\frac{1}{2!} \sum_{i=1}^n \sum_{j=1}^n$
                    $f_{x_ix_j}(a)(x_j-a_j)(x_i-a_i)$

            \hspace{1.7cm}
            + ... +
            $\frac{1}{k!} \sum_{i_1,...,i_k=1}^n
                f_{x_{i_1}...x_{i_k}}(a)(x_{i_1} - a_{i_1})...(x_{i_k} - a_{i_k})$
            + R(a)
    \end{proof}

    \newpage



    \begin{example}
        Let f(x,y,z) = $e^x \sin(yz)$.
        
        Approximate f(0.1,$\frac{\pi}{2}+0.01$,1.05)
        using a 2nd degree Taylor Series.
    \end{example}

    \begin{tbox}
        f(0,$\frac{\pi}{2}$,1)
        = 1

        Df(0,$\frac{\pi}{2}$,1)
        = $\begin{bmatrix}
            e^x \sin(yz)
            & z e^x \cos(yz)
            & y e^x \cos(yz)
        \end{bmatrix}$
        = $\begin{bmatrix}
            1 & 0 & 0
        \end{bmatrix}$

        $D^2$f(0,$\frac{\pi}{2}$,1)
        = $\begin{bmatrix}
            e^x \sin(yz)
            & z e^x \cos(yz)
            & y e^x \cos(yz) \\

            z e^x \cos(yz)
            & -z^2 e^x \sin(yz)
            & -yz e^x \sin(yz) \\

            y e^x \cos(yz)
            & -yz e^x \sin(yz)
            & -y^2 e^x \sin(yz)
        \end{bmatrix}$
        = $\begin{bmatrix}
            1 & 0 & 0 \\
            0 & -1 & -\frac{\pi}{2} \\
            0 & -\frac{\pi}{2} & -\frac{\pi^2}{4}
        \end{bmatrix}$

        f(0.1,$\frac{\pi}{2}+0.01$,1.05)
        $\approx$ $1 + (1*0.1 + 0*0.01 + 0*0.05)$

        \hspace{4cm}
        + $\frac{1}{2}(1*0.1^2 + 0*0.1*0.01 + 0*0.1*0.05$

                        \hspace{5cm}
                        $+ 0*0.01*0.1 - 1*0.01^2 - \frac{\pi}{2}*0.01*0.05$

                        \hspace{5cm}
                        $+ 0*0.05*0.1 - \frac{\pi}{2}*0.05*0.01
                                        - \frac{\pi^2}{4}*0.05^2)$
                        $\approx$ 1.1011

        Actual f(0.1,$\frac{\pi}{2}+0.01$,1.05) $\approx$ 1.1008
        so the estimate is pretty close.
    \end{tbox}

    \vspace{0.5cm}





\subsection{ Extrema }

    \begin{definition}{Local Extrema}{14cm}
        For f: X $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}$,
        let a $\in$ X.
        
        \hspace{0.5cm}
        f has a {\color{lblue} local minimum}
        at a if there is a $\delta > 0$ such that:

        \hspace{1cm}
        f(x) $\geq$ f(a) for all x $\in$ X where $||x-a|| < \delta$
        
        \hspace{0.5cm}
        f has a {\color{lblue} local maximum}
        at a if there is a $\delta > 0$ such that:

        \hspace{1cm}
        f(x) $\leq$ f(a) for all x $\in$ X where $||x-a|| < \delta$
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Local extrema have a derivative of 0}{14cm}
        For differentiable f: X $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}$,
        let a $\in$ X.

        If f has a local extrema at a, then Df(a) = 0.
    \end{wtheorem}

    \begin{proof}
        Let f have a local maximum at a. The proof for local minimum is
        analogous.

        For any i $\in$ \{1,...,n\}, let F(t) = f($a+te_i$).
        Since F: $\mathbb{R}$ $\rightarrow$ $\mathbb{R}$
        has a local maximum at F(0), then
        0 = F'(0) = $\frac{\partial f}{\partial x_i}(a)$.

        Since Df(a) =
        $
        \begin{bmatrix}
            \frac{\partial f}{\partial x_1}(a)
            & \frac{\partial f}{\partial x_2}(a)
            & ...
            & \frac{\partial f}{\partial x_n}(a)
        \end{bmatrix}
        $,
        then Df(a) = 0.
    \end{proof}

    \vspace{0.5cm}



    \begin{definition}{Critical Point and Saddle Point}{14cm}
        For f: X $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}$,
        let a $\in$ X.

        If Df(a) = 0 or undefined, then a is a {\color{lblue} critical point}.

        \vspace{0.3cm}

        Even if Df(a) = 0, f(a) might not be a local extrema
        since it might be a local maximum along some paths and
        a local minimum along other paths.
        For this type of ambiguity of extrema, a is defined as a
        {\color{lblue} saddle point}. 
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Second Derivative Test}{14cm}
        Let f: X $\subset$ $\mathbb{R}^2$ $\rightarrow$ $\mathbb{R}$
        be $C^2$.

        If a $\in$ X is a critical point of f, then:

        \begin{enumerate}[label=(\alph*), leftmargin=1cm, itemsep=0.1cm]
            \item If $f_{xx}(a)f_{yy}(a) - [f_{xy}(a)]^2$ $>$ 0 and
                $f_{xx}(a)$ $>$ 0, then a is a local minimum

            \item If $f_{xx}(a)f_{yy}(a) - [f_{xy}(a)]^2$ $>$ 0 and
                $f_{xx}(a)$ $<$ 0, then a is a local maximum

            \item If $f_{xx}(a)f_{yy}(a) - [f_{xy}(a)]^2$ $<$ 0,
                then a is saddle point
        \end{enumerate}
    \end{wtheorem}

    \vspace{0.5cm}



    \begin{example}
        Let f(x,y) = $3x^2y + y^3 - 3x^2 - 3y^2 + 1$.
        Find and label all critical points.
    \end{example}

    \begin{tbox}
        Df(x,y)
        = $\begin{bmatrix}
            6xy - 6x
            & 3x^2 + 3y^2 - 6y
        \end{bmatrix}$
        = $\begin{bmatrix}
            0 & 0 
        \end{bmatrix}$.

        Thus, 4x(y-1) = 0 so x = 0 or y = 1.

        Thus, $3x^2 + 3y^2 - 6y$ = 0 for (x,y) = (0,0), (0,2), (1,1), and (-1,1).

        $D^2$f(x,y)
        = $\begin{bmatrix}
            6y - 6 & 6x \\
            6x & 6y - 6
        \end{bmatrix}$

        At (0,0), det($D^2$f(0,0)) = 36 and $f_{xx}(0.0)$ = -6
        so local maximum.

        At (0,2), det($D^2$f(0,0)) = 36 and $f_{xx}(0.0)$ = 6
        so local minimum.

        At (1,1), det($D^2$f(0,0)) = -36 so saddle point.

        At (-1,1), det($D^2$f(0,0)) = -36 so saddle point.
    \end{tbox}

    \vspace{0.5cm}



    \begin{definition}{Compactness}{14cm}
        Let set X $\subset$ $\mathbb{R}^n$.

        \vspace{0.3cm}

        A point x $\in$ X is a {\color{lblue} limit point}
        if for any $\delta > 0$:
        
        \hspace{0.5cm}
        The set of all p where $||p-x|| < \delta$
        contain a p $\in$ X where p $\not =$ x.

        X is closed if all limit point of X are in X.

        \vspace{0.3cm}

        X is {\color{lblue} bounded} if there is a M $\in$ $\mathbb{R}$
        such that for all x $\in$ X:

        \hspace{0.5cm}
        $||x||$ $<$ M

        \vspace{0.5cm}

        Then, X is {\color{lblue} compact} if X is closed and bounded.
    \end{definition}

    \vspace{0.5cm}



    \begin{wtheorem}{Extreme Value Theorem}{14cm}
        If X $\subset$ $\mathbb{R}^n$ is compact and
        f: X $\rightarrow$ $\mathbb{R}$ is continuous, then there are

        $x_{\text{min}},x_{\text{max}}$ $\in$ X such that for all x $\in$ X:

        \hspace{0.5cm}
        f($x_{\text{min}}$) $\leq$ f(x) $\leq$ f($x_{\text{max}}$)
    \end{wtheorem}

    \vspace{0.5cm}



    \begin{example}
        Let f(x,y) = $x^2 - xy + y^2 + 1$ where (x,y) $\in$ [-2,2] $\times$ [-2,2].
        
        Find the smallest and largest f(x,y) over [-2,2] $\times$ [-2,2].
    \end{example}

    \begin{tbox}
        For local extrema:
        
        \hspace{0.5cm}
        Df(x,y)
        = $\begin{bmatrix}
            2x - y & -x + 2y
        \end{bmatrix}$
        = $\begin{bmatrix}
            0 & 0
        \end{bmatrix}$
        so (x,y) = (0,0)
        with f(0,0) = 1.

        For boundary points, take the lines:

        \begin{enumerate}[label=(\alph*), leftmargin=1cm, itemsep=0.1cm]
            \item x = -2 for y $\in$ [-2,2]

                \hspace{0.5cm}
                f(-2,y)
                = $5 + 2y + y^2$
                = $(y+1)^2+4$
                so (x,y) = (-2,-1) with f(-2,-1) = 4

            \item x = 2 for y $\in$ [-2,2]

                \hspace{0.5cm}
                f(2,y)
                = $5 - 2y + y^2$
                = $(y-1)^2+4$
                so (x,y) = (2,1) with f(2,1) = 4

            \item y = -2 for x $\in$ [-2,2]

                \hspace{0.5cm}
                f(x,-2)
                = $x^2 + 2x + 5$
                = $(x+1)^2+4$
                so (x,y) = (-1,-2) with f(-1,-2) = 4

            \item y = 2 for x $\in$ [-2,2]

                \hspace{0.5cm}
                f(x,2)
                = $x^2 - 2x + 5$
                = $(x-1)^2+4$
                so (x,y) = (1,2) with f(1,2) = 4
        \end{enumerate}

        For endpoints,
        f(-2,-2) = 5,
        f(-2,2) = 13,
        f(2,-2) = 13,
        f(2,2) = 5.

        min(f(x,y)) = 1 at (x,y) = (0,0)
        \hspace{0.5cm}
        max(f(x,y)) = 13 at (x,y) = (-2,2),(2,-2)
    \end{tbox}

    \newpage





\subsection[ Lagrange Multipliers ]{ Lagrange Multipliers for Constrained Extrema }

    \begin{wtheorem}{Lagrange Multiplier: Optimization for one constraint}{14cm}
        Let f,g: X $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}$
        be $C^1$ and g(x) = c for constant c $\in$ $\mathbb{R}$.
        
        If f(x) has an extrema at $x_0$ where g($x_0$) = c and
        $\nabla g(x_0)$ $\not =$ 0, then there is a scalar $\lambda$ $\in$
        $\mathbb{R}$ such that:

        \hspace{0.5cm}
        $\nabla f(x_0)$ = $\lambda$ $\nabla g(x_0)$
    \end{wtheorem}

    \vspace{0.5cm}



    \begin{example}
        Let f(x,y,z) = 2xy + 2yz + xz
        where xyz = 4. Find the minimum of f(x,y,z).
    \end{example}

    \begin{tbox}
        $\nabla f(x,y,z)$
        = $\begin{bmatrix}
            2y+z \\
            2x+2z \\
            2y+x
        \end{bmatrix}$
        = $\lambda$ $\nabla g(x,y,z)$
        = $\begin{bmatrix}
            \lambda yz \\
            \lambda xz \\
            \lambda xy
        \end{bmatrix}$

        Then, z-x = $\lambda y(z-x)$ so x = z or y = $\frac{1}{\lambda}$

        \hspace{0.5cm}
        If x = z, then 4x = $\lambda x^2$ so x = 0 or x = $\frac{4}{\lambda}$.

        \hspace{1cm}
        If x = 0, then 2y + 0 = $\lambda 0y$ = 0 so y = 0. Thus, (x,y,z) = (0,0,0).

        \hspace{1cm}
        If x = $\frac{4}{\lambda}$, then
        2y + $\frac{4}{\lambda}$ = $\lambda \frac{4}{\lambda} y$ = 4y
        so y = $\frac{2}{\lambda}$. Thus,
        $(\frac{4}{\lambda},\frac{2}{\lambda},\frac{4}{\lambda})$.

        \hspace{0.5cm}
        If y = $\frac{1}{\lambda}$, then $\frac{2}{\lambda} + z$ = z
        which is impossible.

        Since $\nabla f(0,0,0)$ = 0, then (x,y,z) $\not =$ (0,0,0)
        so the only (x,y,z)
        = $(\frac{4}{\lambda},\frac{2}{\lambda},\frac{4}{\lambda})$.

        Thus, 4 = xyz
        = $\frac{4}{\lambda} \frac{2}{\lambda} \frac{4}{\lambda}$
        = $\frac{32}{\lambda^3}$
        so $\lambda$ = 2.
        
        Thus, (x,y,z) = (2,1,2) and the minimum f(x,y,z) = 4 + 4 + 4 = 12.
    \end{tbox}

    \vspace{0.5cm}



    \begin{wtheorem}{Lagrange Multiplier: Optimization for multiple constraints}{14cm}
        Let f,$g_1,...,g_k$: X $\subset$ $\mathbb{R}^n$ $\rightarrow$ $\mathbb{R}$
        be $C^1$ where k $<$ n and each $g_i$(x) = $c_i$ for constants
        $c_i$ $\in$ $\mathbb{R}$ for i = \{1,...,k\}.
        
        If f(x) has an extrema at $x_0$ where each $g_i$($x_0$) = $c_i$ and
        $\nabla g_i(x_0)$ $\not =$ 0, then there are scalars
        $\lambda_1,...,\lambda_k$ $\in$ $\mathbb{R}$ such that:

        \hspace{0.5cm}
        $\nabla f(x_0)$ = $\lambda_1 \nabla g_1(x_0)
                            + \lambda_2 \nabla g_2(x_0)
                            + ... + \lambda_k \nabla g_k(x_0)$
    \end{wtheorem}




